\chapter{PROJECT OBJECTIVES}
\label{chpt:userrequirements}
% \section{Objectives}
Interviewing 102 stakeholders during a National Science Foundation (NSF) I-Corps\footnote{https://www.nsf.gov/news/special\_reports/i\-corps/} program unveiled valuable insights about the requirements for a software solution in this area. This technology's stakeholders include the end-users of proximal sensors and related personnel (e.g., their managers). The proximal sensor devices include portable X-ray fluorescence spectroscopy (\pxrf{}); visible near-infrared spectroscopy (\visnir{}); laser-induced breakdown spectroscopy (LIBS), such as those manufactured by Olympus, Hitachi, Trimble, Thermo Niton, Elvatech, Spectral Evolution, Panalytical, and SciAps, to name but a few. Therefore, these original equipment manufacturers are also stakeholders because they can be potential customers or collaborators.

Five objectives were discovered after the NSF I-Corps program. \textbf{Objective 1)}, research and development of a set of typical visualizations for chemical measurement data, is essential because it helps to reduce data analysis time. Furthermore, these visualization solutions should also incorporate interactive options that allow the scientists (domain experts) to analyze the underlying data. For instance, soil scientists should be able to use their domain knowledge to explore and find insights about the soils in soil science's aspect using the proposing interactive data visualizations for the multivariate proximal sensor data acquired from soil samples. Currently, there is no specific guidance in the analytics process of the proximal sensor data~\cite{COLLINS2018Guidance}. Therefore, soil/water scientists must build new visualizations to suit particular analysis tasks. They also use conventional tools (e.g., Microsoft Excel) and software applications (e.g., ArcMap/ArcGIS) extensively for the analysis requirements. Still, these general-purpose applications are ill-suited for visualizing chemical measurement data~\cite{Ceneda2017Guidance}.

\begin{table}[h]
\caption {Statistics about common software used.\label{tab:softwares}} 
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|r|r|r|r|r|r|}
\hline
\textbf{Excel} & \textbf{Custom code} & \textbf{ArcGIS/ArcMap} & \textbf{SAS/SPSS/JMP} & \textbf{Custom software} & \textbf{Vendor's software} \\ \hline
41             & 20                               & 17                     & 15                    & 9                             & 4                     \\ \hline
\end{tabular}
}
\end{table}

Specifically, out of 102 stakeholders interviewed, 41 of them are the proximal sensor end-users. There are different types of software involved in this area. However, the listed software types (Table \ref{tab:softwares}) are commonly used by these 41 end-users. There are two broad categories of use for these software applications: statistical analysis and chart generation (to report findings). Not surprisingly, all of the interviewed end-users reported using Microsoft Excel for both of these tasks. Many of them (20 out of 41) often use custom codes (e.g., using R, Python, or MatLab) for these two tasks. Also, many others use ArcGIS/ArcMap primarily for graph and chart generations. Concomitantly, they also use SAS, SPSS, or JMP mainly for statistical analysis. There are also users (primarily from companies or government offices) who use custom-made software for their tasks. 

Additionally, most proximal sensor end-users use the software produced by the original equipment manufacturers (proximal sensor devices) to extract/copy/transfer data from proximal sensor devices to other platforms (e.g., computers or cloud data storage). Only a few of the interviewed end-users use some form of software from the hardware vendors (the original proximal sensor manufacturers) for data analysis purposes. A natural question to ask is why the manufacturers do not provide data analysis and visualization solutions for the data their devices produce. Interviews with original equipment manufacturers (e.g., Olympus, Hitachi, SciAps, and Thermo Fisher) revealed various technical hurdles that encumber them from providing data visualization solutions for their equipment. Two main reasons are the lack of data visualization expertise and the different needs of individual customers (e.g., soil scientists and anthropologists). Thus, it is hard to develop data visualizations that suit the needs of all of their customers. As business-oriented companies, they do not take risks in research and development in this direction.

Therefore, \textbf{objective 2)}, research and development of an intelligent visual recommendation component that provides appropriate and personalized visualizations for chemical analysts, is a key differentiator. This intelligent component allows the software to learn and recommend appropriate visualizations to individual users based on the characteristics of the underlying data and the end user's contexts (e.g., their common visualization uses, recent interactions, user profiles). Personal recommendations have gained success in several fields, such as movie recommendations (e.g., Netflix), commercial product recommendations (e.g., Amazon), and advertisement recommendations (e.g., Google Ads). However, scientific literature regarding personalized visualization recommendations is still limited. In other words, this field is still at the pure research stage and is not ready to produce deployable/marketable products. Most of the current visual recommendations (e.g., Microsoft Excel Recommended Charts) are based on fixed rules/heuristics~\cite{ShowMe} and not continuous learnable components. The rules may include recommending visualizations using data types (e.g., categorical, numerical, ordinal) or graph-based visual features~\cite{outliagnostics, pham2019mtsad} using 2D Scagnostics~\cite{Wilkinson2005} or ScagnosticsJS~\cite{pham2020scagnosticsjs} for web applications or higher number of dimensions.


Quantitatively, interviews with proximal sensor end-users unveiled that they spend approximately 25\% of their time building appropriate data analysis and visualization tools for their multivariate proximal sensor data. Moreover, data visualizations are often stressful for users without visualization expertise. For instance, it is hard for them to find appropriate visualizations for specific data types under analysis. Especially, proximal sensor data are often multivariate and complex. Furthermore, they often need to use separate software for analysis/exploration purposes before generating visualizations. The software used for analysis is often unintuitive due to the lack of appropriate visualizations to support analysis tasks. Also, the generated graphics are often static because they do not provide interactive options. This lack of interactive options coupled with intuitive visualizations encumbers the exploration of insights from the multivariate proximal sensor data. Therefore, deliverables of objectives 1 and 2 help reduce the time to generate data visualization solutions (projected at 25\%) and user stress regarding analyzing and reporting their data.


Proximal sensor users also confer that approximately 5\% of the sampling times they need to go back to the fields to resample the collected data due to the absence of error indication while they are still on site. For instance, soil scientists often need to go to fields far from their office, excavate a soil profile, scan the soil in that pedon, bring back the data to their offices and analyze. The collected data might have errors revealed during the analysis time at the office. There are various reasons for mistakes to happen. An example of this is improper positioning of the hardware while scanning, allowing air attenuation of fluoresced X-rays and, subsequently, incorrect readings. Other stories from users also indicate different common error types, such as inadvertently scanning tiny pieces of unwanted materials at the scanning points (e.g., tiny rocks, metal pieces, or even paint chips right at the scanning point). 


Once there is an error in the scanning process, scientists must go back to the field and resample the pedon. In many cases, going back to the area is time-consuming or even impossible. For instance, the National Aeronautics and Space Administration (NASA) is currently using related X-ray fluorescence and \visnir{} spectroscopy sensors on its Mars rovers. NASA scientists with missions on Mars would not make another trip for their equipment to get back to Mars to resample the incorrectly sampled data. The interviewed soil survey staff from the NRCS, USDA (Natural Resources Conservation Service, the United States Department of Agriculture) contributed another example. Specifically, they stated that in several cases, farmers would not allow them to visit and excavate their land multiple times for the reason that errors occurred in the previous pedon scanning process. Thus, as a common practice, scientists often do multiple scans per profile. Consequently, \textbf{objective 3)}, research and development of real-life error indications for proximal sensors, reduces the need for resampling or duplicated scanning efforts (projected at 5\%).


Different end-users scan for elemental concentrations from different materials. For instance, soil scientists work with soil, while anthropologists might be interested in a specific rock such as obsidian. Users often need to purchase different calibration packages or develop their calibration packages for a few expert/capable users. Most current manufacturers use conventional, low-level spectrum analysis algorithms to calibrate the concentration data for different material types. By comparison, conventional machine learning (ML) or deep learning (DL) produces state-of-the-art results in various fields. Thus, initial communications with the Chief Executive Officers (CEOs) and product development teams from the proximal sensor manufacturers (e.g., Olympus, SciAps, Hitachi, and Thermo Fisher) uncovered strong interest in using ML or DL to predict elemental concentrations (or other output types from the proximal sensors) using the raw data acquired by proximal sensors.


Therefore, for \textbf{objective 4)}, research and development of ML/DL components for device calibrations (e.g., spectrum to concentrations or other output types). This work proposes to use existing spectral and elemental concentration data, processed using conventional spectrum processing algorithms, to train models to predict the concentrations from the spectral data in the future. There are two purposes for this: accuracy improvement and reduced processing time.


In the same vein, \textbf{objective 5)}, research and development of ML/DL components with an emphasis on predicting high-level, more difficult-to-measure features using low-level, relatively easy-to-collect proximal sensor data, is equally important. For instance, in the field of soil/water property predictions, discussions with proximal sensor users suggest that they prefer to have ML/DL models to predict higher-level pedological/lithographic characteristics (e.g., soil organic matter, pH, exchangeable Calcium) from spectral data acquired by proximal sensor devices (e.g., \pxrf{} spectroscopy or \visnir{} spectroscopy). Initial work in the literature shows that these higher-level properties of interest cannot be acquired directly from the available proximal sensor technologies with ease. However, they are highly correlated with the underlying spectral data that the proximal sensor devices can acquire. Thus there is an opportunity to leverage ML/DL's predictive power for this task. These ML/DL models can replace the need to go through time-consuming, expensive, and destructive (due to the involvement of environmentally unfriendly chemicals) laboratory procedures to quantify these properties.


% \section{Methods}

The scope of this project with these five objectives is enormous. This dissertation focuses on initial research foundations completed on these five objectives and does not claim to have completed all five objectives. Precisely, the following sections discuss literature and implementations related to a set of common interactive visualizations for soil profile analysis with indications if potential errors exist in the underlying data (objectives 1 and 3), static and dynamic approaches for visual recommendations (objective 2), and ML/DL methods for predicting soil properties from spectral data acquired by proximal sensors (objective 5), and potentials of extending the same ML/DL methods for device calibration (objective 4).